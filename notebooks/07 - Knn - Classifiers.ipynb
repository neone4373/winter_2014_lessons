{
 "metadata": {
  "name": "",
  "signature": "sha256:643a0775dbfa8ed76ab5a92d68376fe2a401c1805ca7b9509a2eb1110b9d2aad"
 },
 "nbformat": 3,
 "nbformat_minor": 0,
 "worksheets": [
  {
   "cells": [
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "# Questions From Last Time?\n",
      "\n",
      "\n",
      "# Lightning Talks 1/26\n",
      "Format 3-5 Minutes of talk and then feedback from Katie and I\n",
      "\n",
      "## What to cover:\n",
      "* Who are you?\n",
      "* What is your topic?\n",
      "* Why are you inspired by this?\n",
      "* How are you approaching the problem?\n",
      "* ** Where will you get your data? **\n",
      "\n",
      "# Resources\n",
      "\n",
      "## Additional Data\n",
      "https://github.com/neone4373/winter_2014_lessons/blob/master/data_sources.md\n",
      "\n",
      "## More info on Final Projects\n",
      "https://github.com/neone4373/winter_2014_lessons/blob/master/FinalProjects.md"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "> When you are a grown up your brothers become your neighbors and your unconditional brotherhood become your conditional neighborhood.\n",
      "\n",
      "<footer>~ Amit Kalantri</footer>"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "![break](assets/agenda.svg)"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "1. [Classification Problems](#Classification-Problems)\n",
      "1. [Building Effective Classifiers](#Building-Effective-Classifiers)\n",
      "1. [KNN Classification](#KNN-Classification)\n",
      "\n",
      "**Labs**\n",
      "1. [Implemeneting the KNN Classification Model](#Implementing-the-KNN-Classification)"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "# KNN Classification"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "![break](assets/theory.svg)"
     ]
    },
    {
     "cell_type": "heading",
     "level": 2,
     "metadata": {},
     "source": [
      "Classification Problems"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "|   \t|continuous\t|categorical   \t|\n",
      "|:-:\t|:-:\t|:-:\t|\n",
      "|**supervised**   \t|regression   \t|**classification**   \t|\n",
      "|**unsupervised**   \t|dimension reduction   \t|clustering   \t|"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "Here\u2019s (part of) an example dataset (Fisher\u2019s Iris Data Set)"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "import pandas as pd\n",
      "from sklearn.datasets import load_iris\n",
      "\n",
      "load_iris?"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "iris = load_iris()\n",
      "df = pd.DataFrame(iris.data, columns=iris.feature_names)\n",
      "df['species'] = [iris.target_names[x] for x in iris.target]\n",
      "df.head(10)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "print 'Independent Variables: \\n%s' % iris.feature_names"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "print 'Class Labels: \\n%s' % iris.target_names"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "**Supervised learning** (machine learning) takes a known set of input data and known responses to the data, and seeks to build a predictor model that generates reasonable predictions for the response to new data."
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "![](assets/classification__mapping_task.png)"
     ]
    },
    {
     "cell_type": "heading",
     "level": 3,
     "metadata": {},
     "source": [
      "Classification in 4 Steps "
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "1)  Split Dataset"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "![](assets/classification_step1.png)"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "2) Train Model"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "![](assets/classification_step2.png)"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "3) Test Model"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "![](assets/classification_step3.png)"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "4) Make Predictions"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "![](assets/classification_step4.png)"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "**This new data is called out of sample data. The labels for these Out of Sample (OOS) records are unknown**"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "All Together Now\n",
      "\n",
      "![](assets/classification_steps.png)"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "![break](assets/theory.svg)"
     ]
    },
    {
     "cell_type": "heading",
     "level": 2,
     "metadata": {},
     "source": [
      "Building Effective Classifiers"
     ]
    },
    {
     "cell_type": "heading",
     "level": 3,
     "metadata": {},
     "source": [
      "What types of prediction error will we run into?"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "1) Training Error"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "![](assets/classification_error_1.png)"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "2) Test / Generalization Error"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "![](assets/classification_error_2.png)"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "3) OOS Error"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "![](assets/classification_error_3.png)"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "**We want to estimate OOS prediction error so we know what to expect from our model.**"
     ]
    },
    {
     "cell_type": "heading",
     "level": 3,
     "metadata": {},
     "source": [
      "Training Error"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "Training error is the average loss over the training data."
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "**Q: Why should we use training & test sets?**\n",
      "\n",
      "> Thought experiment:\n",
      "        \n",
      "Suppose instead, we train our model using the entire dataset.\n",
      "\n",
      "**Q: How low can we push the training error?**\n",
      "    \n",
      "> We can make the model arbitrarily complex (effectively \u201cmemorizing\u201d the entire training set).\n",
      "\n",
      "A: Down to zero! This phenomenon is called overfitting."
     ]
    },
    {
     "cell_type": "heading",
     "level": 3,
     "metadata": {},
     "source": [
      "Overfitting"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "![](assets/overfitting.png)"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "![](assets/overfitting_underfitting.jpg)"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "![](assets/overfitting.svg)"
     ]
    },
    {
     "cell_type": "heading",
     "level": 3,
     "metadata": {},
     "source": [
      "Test / Generalisation Error"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "The expected error over an independent test sample drawn from the same distribution as that of the training data."
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "Training error is not a good estimate of OOS accuracy. Suppose we do the train/test split.\n",
      "\n",
      "**Q: How well does generalization error predict OOS accuracy?**\n",
      "\n",
      "> Thought experiment:\n",
      "\n",
      "Suppose we had done a different train/test split.\n",
      "\n",
      "** Q: Would the generalization error remain the same?** \n",
      "\n",
      "A: Of course not!\n",
      "\n",
      "A: So on its own, generalization error does not predict OOS accuracy very well. It just gives a high-variance estimate of OOS accuracy.\n",
      "\n",
      "> Thought experiment:\n",
      "\n",
      "Different train/test splits will give us different generalization errors.\n",
      "\n",
      "**Q: What if we did a bunch of these and took the average?**\n",
      "\n",
      "A: Now you\u2019re talking! Cross-validation! \n"
     ]
    },
    {
     "cell_type": "heading",
     "level": 3,
     "metadata": {},
     "source": [
      "Cross-validation"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "Out of Sample error is the error when using the trained model to predict instances it hasn't seen before. Often predicted by taking the mean of Cross-Validated test errors."
     ]
    },
    {
     "cell_type": "heading",
     "level": 4,
     "metadata": {},
     "source": [
      "Steps for n-fold cross-validation:"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "1. Randomly split the dataset into n equal partitions.\n",
      "1. Use partition 1 as test set & union of other partitions as training set.\n",
      "1. Find generalization error.\n",
      "1. Repeat steps 2-3 using a different partition as the test set at each iteration.\n",
      "1. Take the average generalization error as the estimate of OOS accuracy."
     ]
    },
    {
     "cell_type": "heading",
     "level": 4,
     "metadata": {},
     "source": [
      "Features of n-fold cross-validation:"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "1. More accurate estimate of OOS prediction error.\n",
      "1. More efficient use of data than single train/test split.\n",
      "\t1. Each record in our dataset is used for both training and testing.\n",
      "1. Presents tradeoff between efficiency and computational expense.\n",
      "\t1. 10-fold CV is 10x more expensive than a single train/test split\n",
      "1. Can be used for model selection."
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "![break](assets/theory.svg)"
     ]
    },
    {
     "cell_type": "heading",
     "level": 2,
     "metadata": {},
     "source": [
      "KNN Classification"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "Linear regression is an example of a parametric method; k-nearest-neighbor is an example of a nonparametric method."
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "Suppose we want to predict the color of the grey dot."
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "1) Pick a value for k."
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "![](assets/knn_step1.png)"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "To prevent ties, one typically uses an odd choice of k for binary classification."
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "2) Find colors of k nearest neighbors."
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "![](assets/knn_step2.png)"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "3) Assign the most common color to the grey dot."
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "![](assets/knn_step3.png)"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "**Our definition of \u201cnearest\u201d implicitly uses the Euclidean distance function.**"
     ]
    },
    {
     "cell_type": "heading",
     "level": 3,
     "metadata": {},
     "source": [
      "Euclidian Distance"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "We measure distance using the Euclidean distance function."
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "$$ d(p,q) = \\sqrt{(p_n -q_n)^2}$$"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "That is, for each feature of data, we\u2019d measure the distance between two observations."
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "Consider the iris data set\u2019s four features, **Sepal length**/**width** and **petal length**/**width**:"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "$$ d(p, q) = \\sqrt{(p_{s.length} - q_{s.length})^2 + (p_{s.width} - q_{s.width})^2  + (p_{p.length} - q_{p.length} ) 2 + (p_{p.width} - q_{p.width})^2} $$"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "$$ d(p, q) = \\sqrt{(5.1 - 4.9)^2 + (3.5 - 3.0)^2  + (1.4 - 1.4)^2 + (.2 - .2)^2} $$"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "$$ d(p, q) = \\sqrt{(.04 + .25 + 0 +0}) = 0.53 $$"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "Euclidean distance is typical for continuous variables, but other metrics can be used for categorical data. If distance measurement is interesting to you, learn more about **taxicab geometry** (the **L1 norm** from regressions last week!) and the **Minkowski distance**.\n",
      "\n",
      "For a completely different metric, also learn about how computer scientists use the **Levenshtein distance**!"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "![break](assets/code.svg)"
     ]
    },
    {
     "cell_type": "heading",
     "level": 2,
     "metadata": {},
     "source": [
      "Implementing the KNN Classification"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "* Apply the KNN Algorithm\n",
      "* Using an RNG to cross validate performance\n",
      "* Compare results to Logistic Regression"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "# %matplotlib inline\n",
      "import pandas as pd\n",
      "import numpy as np\n",
      "# import matplotlib.pyplot as plt\n",
      "\n",
      "# Don't show deprecation warnings\n",
      "import warnings\n",
      "warnings.filterwarnings(\"ignore\", category=DeprecationWarning) \n",
      "\n",
      "# Set some Pandas options\n",
      "pd.set_option('max_columns', 30)\n",
      "pd.set_option('max_rows', 20)\n",
      "\n",
      "# Set some Matplotlib options\n",
      "# matplotlib.rcParams.update({'font.size': 20})\n",
      "\n",
      "# Store data in a consistent place\n",
      "DATA_DIR = '../data/'"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "The best data set to validate any classification algorithm's performance is the [Fisher Iris data set](http://en.wikipedia.org/wiki/Iris_flower_data_set), which is commonly included in any stats or machine learning package."
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "from matplotlib.colors import ListedColormap\n",
      "from sklearn import neighbors, datasets, feature_selection"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "# Various variables we'll need to set intially.\n",
      "n_neighbors = range(1, 51, 2)\n",
      "np.random.seed(32)\n",
      "# Try this sequence again with the following random seed.\n",
      "# observe how it changes the scores of K quite dramatically\n",
      "# np.random.seed(1234)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "# Load in the data and seperate the class labels and input data\n",
      "iris = datasets.load_iris()\n",
      "X = iris.data\n",
      "y = iris.target"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "# Create the training (and test) set using the rng in numpy\n",
      "n = int(len(y) * .7)\n",
      "ind = np.hstack((np.ones(n, dtype=np.bool), np.zeros(len(y) - n, dtype=np.bool)))\n",
      "np.random.shuffle(ind)\n",
      "X_train, X_test = X[ind], X[ind == False]\n",
      "y_train, y_test = y[ind], y[ind == False]"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "# Or more concisely\n",
      "idx = np.random.uniform(0, 1, len(X)) >= 0.3\n",
      "X_train, X_test = X[idx], X[idx==False]\n",
      "y_train, y_test = y[idx], y[idx==False]"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "# Loop through each neighbors value from 1 to 51 and append\n",
      "# the scores\n",
      "scores = []\n",
      "for n in n_neighbors:\n",
      "    clf = neighbors.KNeighborsClassifier(n)\n",
      "    clf.fit(X_train, y_train)\n",
      "    scores.append(clf.score(X_test, y_test))"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "plt.figure(figsize=(20,8))\n",
      "plt.plot(n_neighbors, scores, linewidth=3.0)\n",
      "# plt.show()"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "heading",
     "level": 3,
     "metadata": {},
     "source": [
      "Application of Cross Validation"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "The work above shows that at 11 neighbors, we can get an ideal result that doesn't overfit the data. To verify this, we'll use cross validation."
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "scores = []\n",
      "ind = np.random.uniform(0, 1, len(X)) >= 0.3\n",
      "for n in range(5):\n",
      "    np.random.shuffle(ind)\n",
      "    X_train, X_test = X[ind], X[ind == False]\n",
      "    y_train, y_test = y[ind], y[ind == False]\n",
      "    clf = neighbors.KNeighborsClassifier(11, weights='uniform')\n",
      "    clf.fit(X_train, y_train)\n",
      "    scores.append(clf.score(X_test, y_test))"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "scores"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "np.mean(scores)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "To showcase our whole model's performance, we can plot our algorithm against the two most significant features available in this data set."
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "# Below returns highest signifiance for features 2 and 3\n",
      "# (remember, Python uses index 0). \n",
      "n = np.arange(len(iris.feature_names))\n",
      "\n",
      "fig = plt.figure(figsize=(20,8))\n",
      "ax = fig.add_subplot(111)\n",
      "\n",
      "ax.bar(n, feature_selection.f_classif(X, y)[0])\n",
      "\n",
      "xtickNames = ax.set_xticklabels(iris.feature_names)\n",
      "ax.set_xticks(n)\n",
      "plt.setp(xtickNames, rotation=45, fontsize=16)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "h = .02  # step size in the mesh\n",
      "# Create color maps\n",
      "cmap_light = ListedColormap(['#FFAAAA', '#AAFFAA', '#AAAAFF'])\n",
      "cmap_bold = ListedColormap(['#FF0000', '#00FF00', '#0000FF'])"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "clf = neighbors.KNeighborsClassifier(11, weights='uniform')\n",
      "clf.fit(X[:, 2:4], y)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "# Plot the decision boundary. For that, we will assign a color to each\n",
      "# point in the mesh [x_min, m_max]x[y_min, y_max].\n",
      "x_min, x_max = X[:, 2].min() - 1, X[:, 2].max() + 1\n",
      "y_min, y_max = X[:, 3].min() - 1, X[:, 3].max() + 1"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "xx, yy = np.meshgrid(np.arange(x_min, x_max, h), np.arange(y_min, y_max, h))\n",
      "\n",
      "Z = clf.predict(np.c_[xx.ravel(), yy.ravel()])"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "# Put the result into a color plot\n",
      "Z = Z.reshape(xx.shape)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "plt.figure(figsize=(20,8))\n",
      "plt.pcolormesh(xx, yy, Z, cmap=cmap_light)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "# Plot also the training points\n",
      "plt.figure(figsize=(20,8))\n",
      "plt.pcolormesh(xx, yy, Z, cmap=cmap_light)\n",
      "plt.scatter(X[:, 2], X[:, 3], c=y, cmap=cmap_bold)\n",
      "plt.xlim(xx.min(), xx.max())\n",
      "plt.ylim(yy.min(), yy.max())\n",
      "plt.title(\"3-Class classification (k = %i, weights = '%s')\"\n",
      "         % (21, 'uniform'))"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "heading",
     "level": 4,
     "metadata": {},
     "source": [
      "Normalisation"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "df = pd.DataFrame(iris.data, columns=iris.feature_names)\n",
      "df = df.ix[:,:4]\n",
      "df.describe()"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "\"\"\"\n",
      "Normalise a dataframe, centered around 0\n",
      "\"\"\"\n",
      "\n",
      "df_norm = (df - df.mean()) / (df.max() - df.min())"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "df_norm.head()"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "df_norm.describe()"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "\"\"\"\n",
      "Normalise a set of columns in a dataframe, between 0 and 1\n",
      "\"\"\"\n",
      "\n",
      "df_norm = (df - df.min()) / (df.max() - df.min())"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "df_norm.head()"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "df_norm.describe()"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "\"\"\"\n",
      "Weight a set of columns in a dataframe, by 2 and 1/2\n",
      "\"\"\"\n",
      "\n",
      "sepals = ['sepal length (cm)','sepal width (cm)']\n",
      "petals = ['petal length (cm)','petal width (cm)']\n",
      "df_weighted = pd.DataFrame.join(df[sepals] * 2, df[petals] / 2)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "df_weighted.head()"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "df_weighted.describe()"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "heading",
     "level": 4,
     "metadata": {},
     "source": [
      "Parallel Coordinates"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "from pandas.tools.plotting import parallel_coordinates\n",
      "\"\"\"\n",
      "Parallel Coordinates plot of the Iris Dataset\n",
      "\"\"\"\n",
      "\n",
      "matplotlib.rcParams.update({'font.size': 24})\n",
      "\n",
      "\n",
      "df = pd.DataFrame(iris.data, columns=iris.feature_names)\n",
      "species = [iris.target_names[x - 1] for x in iris.target]\n",
      "df = df.join(pd.DataFrame(species, columns=['Species']))\n",
      "\n",
      "plt.figure(figsize=(20,8))\n",
      "parallel_coordinates(df, 'Species', colormap='gist_rainbow')\n",
      "plt.show()"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "import matplotlib.pyplot as plt\n",
      "import matplotlib.ticker as ticker\n",
      "\n",
      "\"\"\"\n",
      "Hacked version of Parallel Coordinates, which allows for scaled axis\n",
      "\"\"\"\n",
      "\n",
      "def parallel_coordinates(data_sets, style=None):\n",
      "    dims = len(data_sets[0])\n",
      "    x    = range(dims)\n",
      "    \n",
      "    fig, axes = plt.subplots(1, dims-1, sharey=False, figsize=(20, 8))\n",
      "    \n",
      "    if style is None:\n",
      "        style = ['r-']*len(data_sets[0])\n",
      "\n",
      "    # Calculate the limits on the data\n",
      "    min_max_range = list()\n",
      "    for m in zip(*data_sets):\n",
      "        mn = min(m)\n",
      "        mx = max(m)\n",
      "        if mn == mx:\n",
      "            mn -= 0.5\n",
      "            mx = mn + 1.\n",
      "        r  = float(mx - mn)\n",
      "        min_max_range.append((mn, mx, r))\n",
      "\n",
      "    # Normalize the data sets\n",
      "    norm_data_sets = list()\n",
      "    for ds in data_sets:\n",
      "        nds = [(value - min_max_range[dimension][0]) / \n",
      "                min_max_range[dimension][2] \n",
      "                for dimension,value in enumerate(ds)]\n",
      "        norm_data_sets.append(nds)\n",
      "    data_sets = norm_data_sets\n",
      "\n",
      "    # Plot the datasets on all the subplots\n",
      "    for i, ax in enumerate(axes):\n",
      "        for dsi, d in enumerate(data_sets):\n",
      "            s = int(dsi / 50)\n",
      "            ax.plot(x, d, style[s])\n",
      "        ax.set_xlim([x[i], x[i+1]])\n",
      "\n",
      "    # Set the x axis ticks \n",
      "    for dimension, (axx,xx) in enumerate(zip(axes, x[:-1])):\n",
      "        axx.xaxis.set_major_locator(ticker.FixedLocator([xx]))\n",
      "        ticks = len(axx.get_yticklabels())\n",
      "        labels = list()\n",
      "        step = min_max_range[dimension][2] / (ticks - 1)\n",
      "        mn   = min_max_range[dimension][0]\n",
      "        for i in xrange(ticks):\n",
      "            v = mn + i*step\n",
      "            labels.append('%4.2f' % v)\n",
      "        axx.set_yticklabels(labels)\n",
      "\n",
      "\n",
      "    # Move the final axis' ticks to the right-hand side\n",
      "    axx = plt.twinx(axes[-1])\n",
      "    dimension += 1\n",
      "    axx.xaxis.set_major_locator(ticker.FixedLocator([x[-2], x[-1]]))\n",
      "    ticks = len(axx.get_yticklabels())\n",
      "    step = min_max_range[dimension][2] / (ticks - 1)\n",
      "    mn   = min_max_range[dimension][0]\n",
      "    labels = ['%4.2f' % (mn + i*step) for i in xrange(ticks)]\n",
      "    axx.set_yticklabels(labels)\n",
      "\n",
      "    # Stack the subplots\n",
      "    \n",
      "    plt.subplots_adjust(wspace=0,)\n",
      "    \n",
      "    return plt\n",
      "\n",
      "matplotlib.rcParams.update({'font.size': 16})\n",
      "\n",
      "\n",
      "parallel_coordinates(iris.data, ['c','m','y'])"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "![break](assets/voronoi.svg)"
     ]
    },
    {
     "cell_type": "heading",
     "level": 3,
     "metadata": {},
     "source": [
      "Classwork"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "Download the iPython notebook dat17-lab04.ipynb to work through the data visualization introduction, reproduce the KNN implementation about and use that to answer the two exersize questions."
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "![break](assets/mountain.svg)"
     ]
    },
    {
     "cell_type": "heading",
     "level": 2,
     "metadata": {},
     "source": [
      "Discussion"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "1. What are some potential setbacks or pitfalls to the KNN algorithm?\n",
      "2. What are some potential implementation changes to the algorithm that\n",
      "could be made to get around these pitfalls?"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "* What if features aren't same units?\n",
      "* What if the data isn't normalised?\n",
      "* What if one feature is more important than the other?"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "#### Advantages\n",
      "\n",
      "* Very simple implementation.\n",
      "* Robust with regard to the search space; for instance, classes don't have to be linearly separable.\n",
      "* Classifier can be updated online at very little cost as new instances with known classes are presented.\n",
      "* Few parameters to tune: distance metric and k."
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "#### Disadvantages\n",
      "\n",
      "* Expensive testing of each instance, as we need to compute its distance to all known instances. Specialized algorithms and heuristics exist for specific problems and distance functions, which can mitigate this issue. This is problematic for datasets with a large number of attributes. When the number of instances is much larger than the number of attributes, a R-tree or a kd-tree can be used to store instances, allowing for fast exact neighbor identification.\n",
      "* Sensitiveness to noisy or irrelevant attributes, which can result in less meaningful distance numbers. Scaling and/or feature selection are typically used in combination with kNN to mitigate this issue.\n",
      "* Sensitiveness to very unbalanced datasets, where most entities belong to one or a few classes, and infrequent classes are therefore often dominated in most neighborhoods. This can be alleviated through balanced sampling of the more popular classes in the training stage, possibly coupled with ensembles."
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "![break](assets/resources.svg)"
     ]
    },
    {
     "cell_type": "heading",
     "level": 2,
     "metadata": {},
     "source": [
      "Resources"
     ]
    },
    {
     "cell_type": "heading",
     "level": 4,
     "metadata": {},
     "source": [
      "Articles"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "* [KNN in Simple Terms](http://www.jiaaro.com/KNN-for-humans/)\n",
      "* [Should I normalize/standardize/rescale?](http://www.faqs.org/faqs/ai-faq/neural-nets/part2/section-16.html)\n",
      "* [Mahalanobis distance](http://en.wikipedia.org/wiki/Mahalanobis_distance)"
     ]
    },
    {
     "cell_type": "heading",
     "level": 4,
     "metadata": {},
     "source": [
      "Tools"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "* [KNN Algorythms in SciKit Learn](http://scikit-learn.org/stable/modules/neighbors.html)\n",
      "* [Datasets provided by SciKit Learn](http://scikit-learn.org/stable/datasets/)\n",
      "* [Matplotlib Colormaps](http://wiki.scipy.org/Cookbook/Matplotlib/Show_colormaps)\n",
      "* [Model evaluation](http://scikit-learn.org/stable/modules/model_evaluation.html)\n"
     ]
    },
    {
     "cell_type": "heading",
     "level": 4,
     "metadata": {},
     "source": [
      "Code"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "* [Intuitive Classification using KNN and Python](http://blog.yhathq.com/posts/classification-using-knn-and-python.html)\n"
     ]
    }
   ],
   "metadata": {}
  }
 ]
}